---
title: "Data Science: Capstone"
author: "Robert E Lee Lewis"
date: "June 8, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
### MovieLens Project Submission
The purpose of this project (final test) is to create an algorithm for predicting movie ratings and calculating the RMSE using the provided data. The dataset comes from Grouplen dataset, Movielens and consists of two files, movie.dat containing 32043 different movie titles and ratings.dat with approximately 10 million user movie ratings. 

My first thought about predicting movie rating, is to utilize a collaborative filtering approachs i.e: User based, Item based collaborative filtering and POPLAR. 

This project began with a few challenges, first is to overcome beginning with acually loading the data for analysis and then memory issues. The first real challenge I had was extracting the ratings data from the ml-10m100K/ratings.dat file which I could not complete on my laptop (Alienware I7 6700hq cpu @2.60Ghz and 16GB with Windows 10 Pro for Workstations, of which I still have no idea why it will not import). After days of attempting then purchasing a faster desktop computer then I was able to sucessfully import. During my initial struggles I also found that using fread function compared to read.table function to be faster considerable faster for reading in the data therefore I altered the initial download process from what was given.

Two tables of data called movies and ratings are provided. The datasets will be joined by movieIds and userIDs to make our MovieLens Dataset. The MovieLens data will then be split with 10 percent as Validation dataset and the remainder as EDX dataset. Make sure userId and movieId in validation set are also in edx set then add the Validation set back into EDX.

```{r Install Packages and libraries, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(useful)) install.packages("useful", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(recommenderlab)) install.packages("recommenderlab", repos = "http://cran.us.r-project.org")
library(dplyr)
library(stringr)
library(tidyverse)
library(caret)
library(data.table)
library(useful)
library(lubridate)
library(recommenderlab)
```
```{r Load Data, echo=FALSE}
cat("Movielens Preptime Times:")  
system.time({
# MovieLens 10M dataset:
if(!file.exists("ml-10m.zip")){
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", destfile = "ml-10m.zip")
}
unzip("ml-10m.zip", exdir = "movies")
#dir("movies")
#dir("movies/ml-10m100K/")
ratings  <- fread("ml-10m100K/ratings.dat", sep = ":")[, c(1,3,5,7), with=FALSE]
setnames(ratings, c("userId","movieId","rating", "timestamp"))

movies <- str_split_fixed(readLines("ml-10m100K/movies.dat"), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm( ratings, movies, test_index, temp, movielens, removed)  
})
#original code took 20 minutes to complete this initial dataset process
#Modifing the code to use reduces initial dataset process to approx 1 minute.
```

#### Initial Analysis of EDX Data:  
```{r quick review of dataset, echo=FALSE}
edx_rows <- nrow(edx)
edx_cols <- ncol(edx)
edx_names <- names(edx)
glimpse(edx) 
summary(edx)
class(edx)
test_na <- na.omit(edx)
dim(test_na) #it appears there is no missing data in edx
cat("Size of edx file (bytes) : " , object.size(as(edx,"matrix")), "\n")   
cat("Dimensions (rows/columns): " , dim(edx),"\n")  
cat("Glimpse view of edx data  : ")
corner(edx,c = 6) 
```

First review a sample set of edx dataset, using the image of the dataset with selected 100 users and 100 movies. In the image below, each row represents a user and the columns represent movies they rated. 

Note in our sample set that not every user has rated a movie and another user has almost rated every movie. Since I plan to use User based Collaborative Filtering and  Item Based Collabative Filtering for my predictions, I need my good dataset with many users that have rated at least a many movies.

```{r Top 100 movies by rating counts, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

users <- sample(unique(edx$userId), 100)

edx %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey", main = "Image of 100 users and 100 movies rated")
```

Plot edx data for the count of movies rated:

```{r Get Movie rating counts, echo=FALSE}
edx %>% 
  dplyr::count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Movies")
```

Plot the count of ratings given by users:

```{r Get User Rating counts, echo=FALSE}
edx %>% 
  dplyr::count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Users")

```

### Preparing Data:
####Filter EDX for a good set of data
We need a good set of data to create our recommendation models. Based on the information provided in the above graphs we notice that most users rated more than 20 movies. The the previously selected users I filter users that have rated at least 1 top 100 movie.
```{r Preparing data, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
keep_user <- edx %>%
  dplyr::count(userId) %>%
  filter(n >= 20) %>%
  pull(userId)

edx_1 <- edx %>%filter(userId %in% keep_user) 

keep <- edx %>%
  dplyr::count(movieId) %>%
  top_n(100) %>%
  pull(movieId)

edx_1 <- edx_1 %>%filter(movieId %in% keep)  
```
Giving a dataset with **`r length(keep)`** movies and **`r length(keep_user)`** with **`r dim(edx_1)`** ratings and columns.  

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
cat("Size of Prepared Dataset\t", object.size(as(edx_1,"matrix")),"\n")

```
I will be using the recommenderLab library for creating my recomendations therefore I must convert my data to realRatingMatrix class. Converting to realRatingsMatrix requires the data to be converted to a sparse matrix and also greatly reduces the file size.  
1. I used dcast.data.table to cast the data.frame as a table  
2. I used sprintf to convert MovieId's, and UserId's to chr  
3. I used corner to view a small sample of the data to verify conversion  
4. I then convert the data ta a matrix and then a realRatingMatrix  

```{r Convert edx to realRatingsMatrix, echo=FALSE}
edx_2 <- dcast(userId ~ movieId, data = edx_1, value.var = "rating")
dim(edx_2)
class(edx_2)
#view data
require(useful)
  corner(edx_2)

#change rownames
rownames(edx_2) <- sprintf("User%s",edx_2$userId)  
edx_2$userId <- NULL  
corner(edx_2)  

#change column Names
colnames(edx_2) <- sprintf("Movie%s",colnames(edx_2))  
corner(edx_2)  

#convert to matrix  
edx_2 <- as.matrix(edx_2)  
dim(edx_2)  
class(edx_2)  

#convert to realRatingMatrix
edx_3 <- as(edx_2 , "realRatingMatrix")  
class(edx_3)  
str(edx_3)  
```


####Review the prepared dataset  

Review ratings distribution  

```{r Review ratings distribution, echo=FALSE}
hist(getRatings(edx_3), main = "Distribution of Ratings")
```

```{r Normalized Distribution of Ratings, echo=FALSE }
hist(getRatings(normalize(edx_3)),breaks = 100,main = "Normalized Distribution of Ratings")
```

```{r Visual image of distribution of first 500 users, echo=FALSE}
image(edx_3[1:500,],main = "Visual image of distribution of first 500 users")
```

```{r Avg Ratings of Prepared data, echo = TRUE}
boxplot(rowMeans(edx_3),main = "Avg Ratings of Prepared data")  

```  

### Create Model Dataset  

#### Trim Dataset  

Due to memory issues, I remove users with less than 70 movie ratings.  

```{r Create model data, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
model_data <- edx_3[rowCounts(edx_3) > 70]
model_data
boxplot(rowMeans(model_data), main = "Model Data")  
```

### Remove Outliers  
When looking at the previous boxplot of the Model Data we have a few outliers with row means below 2.7 and above 4.6 so I will remove them.

Outiers with rowMeans below 2.7 to remove

```{r Remove outliers, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
dim(model_data[rowMeans(model_data) < 2.7 ])   
```

Outiers wit rowMeans above 4.6 to remove

```{r echo=FALSE}
dim(model_data[rowMeans(model_data) > 4.6 ])   
```
```{r include=FALSE}
model_data <- model_data[rowMeans(model_data) >= 2.7 & rowMeans(model_data) <= 4.6 ]  
```

Rows remaining after removing outliers
```{r Remaing data dimisions, echo=FALSE}
dim(model_data)
```

#### After Outliers removed...  
Boxplot of Model Data after outliers removed, data is symetric.

```{r Boxplot after removin outliers, echo=FALSE}
boxplot(rowMeans(model_data))   

```

Number of Rating remaining in Model Data  

```{r Number of ratings after outliers removed, echo=FALSE}
nratings(model_data)  

```

#### Model dataset Distribution of Ratings  

```{r Plot the Model Data movie ratings, echo=FALSE}
hist(getRatings(model_data), main = "Distribution of Ratings after removing Outliers")   

```

Visual of Model Dataset  

```{r image of Model dataset, echo=FALSE}
image(as(model_data,"matrix"), main = "Visual image of Rating distribution - Model Data")  

```

Model data appears to be well ditributed.   

### Create Recommender Sets  
Divide the Model data set into train and test sets, 80/20 respectivly.  

```{r Create recommender sets, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
set.seed(1)
which_train <- sample( x = c(TRUE, FALSE), size = nrow(model_data), replace = TRUE, prob = c(0.8, 0.2))
head(which_train)
rec_data_train <- model_data[which_train]
rec_data_test <- model_data[!which_train]

```
Recommender Train set diminsions: `r dim(rec_data_train)`  
Recommender Test set diminsions: `r dim(rec_data_test)`  

### Build Models  
I will use various models then compare prediction accuracies to determine the best algorithim. The available models I will use include:  

$IBCF_realRatingMatrix
Recommender method: IBCF for realRatingMatrix
Description: Recommender based on item-based collaborative filtering.
Reference: NA
Parameters:
   k   method normalize normalize_sim_matrix alpha na_as_zero
1 30 "Cosine"  "center"                FALSE   0.5      FALSE

$POPULAR_realRatingMatrix
Recommender method: POPULAR for realRatingMatrix
Description: Recommender based on item popularity.
Reference: NA
Parameters:
  normalize                                                    aggregationRatings
1  "center" new("standardGeneric", .Data = function (x, na.rm = FALSE, dims = 1, 
                                                  aggregationPopularity
1 new("standardGeneric", .Data = function (x, na.rm = FALSE, dims = 1, 

$RANDOM_realRatingMatrix
Recommender method: RANDOM for realRatingMatrix
Description: Produce random recommendations (real ratings).
Reference: NA
Parameters: None

```{r Veiw various recommender models, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
recommender_models <-recommenderRegistry$get_entries(dataType = "realRatingMatrix")  
recommender_models  
lapply(recommender_models,"[[","description")    
```
From the list above I will be using UBCF, IBCF and POPULAR models.

#### User Based Collborative Filtering (UBCF) Model
Collaborative filterin uses algorithims to filter users ratings to make personalized recommendations from similiar users (difinitio n from whatis.techtarget.com/definition/collaborative-filtering )

```{r Build User Based Collabrotive model, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
recc_model <- Recommender(data = rec_data_train, method = "UBCF")  
recc_model  
recc_model@model$data  
```

Get recommendations for testset:
```{r Get recommender test set, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
# note UBCF model is a lazy learner technic that must access all data in order to make a prediction
  n_recommended <- 10
  recc_predicted <- predict(object = recc_model, newdata = rec_data_test, n = n_recommended)
  rec_list <- sapply(recc_predicted@items, function(x){colnames(model_data)[x]})
```

List of recommendation movies for test set users 7 thru 10:  
```{r List recommendations for a few users, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  rec_list[7:10]  

```
Total number of recommendations by users in test set  
```{r Total number of recommenders in test set, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  number_of_items = sort(unlist(lapply(rec_list, length)), decreasing = TRUE)
  table(number_of_items)
```
Note that 428 users from the test set received 10 recommendations. 

Analyze number of ratings per user
```{r Analyze number of ratings per users, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  table(rowCounts(model_data)) 
```
Note that only one user has rated 96 of the top 100 rated movies. 


#### Create an evaluators scheme:
Create evaluation datasets using recommenderLab evaluationScheme function  

```{r Create Evaluation scheme, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  items_to_keep <- 30
  rating_threshold <- 4
  n_fold <- 5
  eval_sets <- evaluationScheme(data = model_data, method = "cross-validation", train 
                                = percentage_training, given = items_to_keep, goodRating = rating_threshold, k = n_fold)
  eval_sets  
```

```{r Get size of evaluation sets, echo=FALSE}
  size_sets <- sapply(eval_sets@runsTrain, length)   
  cat("Sizes of Evaluation Sets:\t", size_sets, "\n")   
  getData(eval_sets, "train")
```
3 sets will be used:
  train = training set  
  known = test set used to build recommendations   
  unknown = test set to test the recommendations   
  
Create UBCF Recommender 
```{r Evaluate UBCF known, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  model_to_evaluate <-"UBCF"
  model_paramenter <- NULL
  eval_recommender <- Recommender(data = getData(eval_sets, "train"), method = model_to_evaluate, parameter = model_paramenter)
  eval_recommender  
```
Get UBCF Filter predictions for known test set  
```{r Get evaluation predictions, echo=FALSE}
  items_to_recommend <- 10
  eval_prediction <- predict(object = eval_recommender, newdata = getData(eval_sets, "known"), n = items_to_recommend, type = "ratings")
  eval_prediction 
```
Calculate the prediction accuracy for each user in unknown test set:
```{r Calculate the prediction accuracy UBCF, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, data = getData(eval_sets, "unknown"), byUser = TRUE)
  head(eval_accuracy)
```
Calculate the overall avgerages in unknown test set: 
```{r Calculate the accuracy of UBCF Model, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  apply(eval_accuracy,2,mean)
```
Calculate the overall accuracy given in unknown test set:
```{r Calculate the UBCF overall accuracy, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, data = getData(eval_sets, "unknown"), byUser = FALSE)
  eval_accuracy
```
Use precicion recall to predict accuracy with confusion matrix for known test set
```{r Use prediction recall to predict accuarcy, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  results <- evaluate(x = eval_sets, method = model_to_evaluate, n = seq(10, 100, 10))
  head(getConfusionMatrix(results)[[1]])
  # note first 4 columns cotain the True False Positives
```
Sum up the UBCF indexes:
```{r Sum up the UBCF indexes, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#sum up the indexes 
  columns_to_sum <- c("TP","FP","FN","TN")
  indicies_summed <- Reduce("+", getConfusionMatrix(results))[,columns_to_sum]
  indicies_summed  
  
```
*Note: it is difficult to visulize the data provided unless the results are plotted.*


Create UBCF Receiver operating characteristic (ROC) plot
```{r Create UBCF ROC plot, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  plot(results, annotate = TRUE, main = "UBCF - ROC Curve")  

```

Plot UBCF Precision/recall to verify accuracy
```{r Plot UBCF Precision recall, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#plot shows the relation ship between TPR and FPR
  #30 TPR is close to 0.7 and the FPR is less than 0.4 is good
  #40 TPR is close to 0.7 but the FPR is greater than 0.4 is not as good
  plot(results, "prec/rec",annotate = 1, main = "UBCF - Precision/recall")   

```
*Note the precision/recall at #30 is not the best at 0.58/0.66*


#### Fine Tuning of the Models to get best results

Create UBCF Models with varing vector_nn and different metods i.e.: cosine and pearson

```{r Fine tune UBCF model, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
vector_nn <- c(5,10,20, 30)
UBCF_cos_model <- lapply(vector_nn, function(nn,l){ list(name ="UBCF", param = list(method = "cosine", nn = nn))})
names(UBCF_cos_model) <- paste0("UBCF_cos_k_", vector_nn)
names(UBCF_cos_model)[1]
UBCF_pea_model <- lapply(vector_nn, function(nn,l){ list(name ="UBCF", param = list(method = "pearson", nn = nn))})
names(UBCF_pea_model) <- paste0("UBCF_pea_k_", vector_nn)
names(UBCF_pea_model)[1]
models <- append(UBCF_cos_model, UBCF_pea_model)
models  

```

Determine the best UBCF results based on number of recommendations  

```{r Get UBCF results, echo=FALSE}
n_recommendations <- c(1, 5, seq(10,100,10))
list_results <- evaluate(x = eval_sets, method = models, n = n_recommendations)  

```

Plot UBCF results 

```{r plot UBCF model results, echo=FALSE}
plot(list_results, annotate = c (1,2), legend = "bottomright")
title("UBCF ROC curve")  

```

*Note: UBCF_pea_k_30 appears to be the best UBCF model with TPR closes to 0.7 and FPR less than 0.4*

```{r echo=FALSE}
plot(list_results, "prec/rec",annotate = 1, legend= "bottomleft")
title("UBCF Precision/recall")
```
*Note: The precision/recall support UBCF_pea_k_30 appears to be the best UBCF model with high persision*


Create IBCF Models with varing vector_nn and different metods i.e.: cosine and pearson
```{r Create IBCF model, echo=FALSE}
vector_k <- c(5,10,20,30)
IBCF_cos_model <- lapply(vector_k, function(k,l){ list(name ="IBCF", param = list(method = "cosine", k = k))})
names(IBCF_cos_model) <- paste0("ICBF_cos_k_", vector_k)
names(IBCF_cos_model)[1]
IBCF_pea_model <- lapply(vector_k, function(k,l){ list(name ="IBCF", param = list(method = "pearson", k = k))})
names(IBCF_pea_model) <- paste0("ICBF_pea_k_", vector_k)
names(IBCF_pea_model)
models <- append(IBCF_cos_model, IBCF_pea_model)
models
```

Get IBCF model results 

```{r Get IBCF results, echo=FALSE}
n_recommendations <- c(1, 5, seq(10,100,10))
list_results <- evaluate(x = eval_sets, method = models, n = n_recommendations)   

```
 
 Plot IBCF results   
 
```{r plot IBCF ROC curve, echo=FALSE}
plot(list_results, annotate = c (1,2), legend = "bottomright")
title("IBCF ROC curve")   
```

```{r plot IBCF Precision/recall, echo=FALSE}
plot(list_results, "prec/rec",annotate = 1, legend= "bottomright")
title("Precision/recall")   
  
```

IBCF model #10-40 TPR is closes to to 0.7 and the FPR is less than 0.4 is good

####Create a POPULAR model   

Create POPULAR model   
```{r Create Popular model, echo=FALSE}
vector_nn <- 30
POP_model <- lapply(vector_nn, function(nn,l){ list(name ="POPULAR")})
names(POP_model) <- "POPULAR"
names(POP_model)[1]
models <- POP_model
models  

```

Get POPULAR Results   

```{r Get Popular list results, include=FALSE}
n_recommendations <- c(1, 5, seq(10,100,10))
list_results <- evaluate(x = eval_sets, method = models, n = n_recommendations)
```

Plot POPULAR Results   

```{r Plot Popular ROC Curve, echo=FALSE}
#plot and choose the optimal parameters
plot(list_results, annotate = c (1,2), legend = "topleft")
title("ROC curve")   

```

```{r Plot Popular Precision/recall, echo=FALSE}
plot(list_results, "prec/rec",annotate = 1, legend= "bottomright")
title("Precision/recall")   
```

#### Combine best results from UBCF, IBCF and POPULAR models to determine my Final Model

```{r include=FALSE}
models <- append(UBCF_pea_model, POP_model )
n_recommendations <- c(1, 5, seq(10,100,10))
list_results <- evaluate(x = eval_sets, method = models, n = n_recommendations)  

```

Plot Best Results   

```{r Plot Best ROC Curve, echo=FALSE}
#plot and choose the optimal parameters
plot(list_results, annotate = c (1,2), legend = "topleft")
title("ROC curve")   

```

```{r Plot Best Precision/recall, echo=FALSE}
plot(list_results, "prec/rec",annotate = 1, legend= "bottomright")
title("Precision/recall")   
```


### Evaluate using the Validation dataset

#### Initial Analysis of Validation data:
Dimensions (rows/columns):
```{r Initial view of Validation data, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
dim(validation)
```
Corner view of Validation data:
```{r Corner view of Validation data, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
corner(validation)
```

#### Convert Validation dataset to matrix then to realRatingMatrix  for use with recommenderlab  
```{r Convert validation to realRatingsMatrix, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
val_movies <- validation %>%
  dplyr::count(movieId) %>%
  top_n(1000) %>%
  pull(movieId)
val_user <- validation %>%
  dplyr::count(userId) %>%
  filter(n > 10) %>%
  pull(userId)
val_1 <- validation %>%filter(userId %in% val_user)
dim(edx_1)
val_1 <- val_1 %>%filter(movieId %in% val_movies)
dim(val_1)
val_2 <- dcast(userId ~ movieId, data = val_1, value.var = "rating")
dim(val_2)

#view data
require(useful)
  corner(val_2)

#change rownames
rownames(val_2) <- sprintf("User%s",val_2$userId)
val_2$userId <- NULL
corner(val_2)

#change column Names
colnames(val_2) <- sprintf("Movie%s",colnames(val_2))
corner(val_2)

#convert to matrix  
val_2 <- as.matrix(val_2)
#head(edx_2)
#class(edx_2)
dim(val_2)
val_3 <- as(val_2, "realRatingMatrix")
val_data <- val_3[rowCounts(val_3) >70]
```
Summary of validation realRatingMatrix dataset:  
```{r echo=FALSE}
cat("Total number of ratings in validation dataset:\t", nratings(val_data),"\n")
cat("Total Users to Movies in validation set:\t", dim(val_data), "\n")   
```

Create Validation Sets
```{r echo=FALSE}
items_to_keep <-30
rating_threshold <- 4
n_fold <- 5
n_recommendations <- c(1, 5, seq(10,100,10))
val_sets <- evaluationScheme(data = val_data, method = "cross-validation", train 
                                = percentage_training, given = items_to_keep, goodRating = rating_threshold, k = n_fold)  
```
Evaluate Validation data with selected models  

Final UBCF Evaluation results:  
```{r Final UBCF Evaluation results, echo=FALSE}
UBCF_eval <- evaluate(x = val_sets, method = "UBCF", k = n_fold, type = "ratings")

```

```{r Final UBCF Results, echo=FALSE}
head(getConfusionMatrix(UBCF_eval)[[1]])  

```

Final IBCF Evaluation results:  
```{r echo=FALSE}
IBCF_eval <- evaluate(x = val_sets, method = "IBCF", k = n_fold, type = "ratings")

```


```{r IBCF Results, echo=FALSE}
head(getConfusionMatrix(IBCF_eval)[[1]])

```

Final POPULAR Evaluation results:  
```{r Final POPULAR Evaluation results, echo=FALSE}
POP_eval <- evaluate(x = val_sets, method = "POPULAR", n = n_recommendations, type = "ratings")

```

```{r Final POPULAR Results, echo=FALSE}
head(getConfusionMatrix(POP_eval)[[1]])
```

### Conclusion

In Conclusion using using the **POPULAR**  model from the recommenderLab library, keeping **30** items with a predicted movie rating of **4** reports an RMSE of **0.872**.
In this capstone prioject I learned various ways to improve speed using different classes of data and different libraries that were not introduced in the class.

```{r File size reduction, echo=FALSE}
cat("The Prepared dataset was reduced by", object.size(as(edx_3, "matrix"))/object.size(edx_3),"to 1 byte when converted from matrix to realRatingsMatrix which greatly reduced time, memory issues and stress.\n")

```


####notes

A downfall User Based Collaborative Filtiering is the user need to have rated a few movies inorder to find similiar users ratings. 


The idea of gathering similiar users based on movies they watched and how they rated the movies then use the total average means to predict the movie ratings.  All this said I begin cleaning the data based on users than have rated alot of movies.  
