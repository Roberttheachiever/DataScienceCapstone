---
title: "Data Science: Capstone"
author: "Robert E Lee Lewis"
date: "June 8, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
### MovieLens Project Submission
The purpose of this project (final test) is to create an algorithm for predicting movie ratings and calculating the RMSE using the provided data. The dataset comes from Grouplen dataset, Movielens and consists of two files, movie.dat containing 32043 different movie titles and ratings.dat with approximately 10 million user movie ratings. 

My first thought about predicting movie rating, is to utilize a collaborative filtering approachs i.e: User based, Item based collaborative filtering and POPLAR. 

This project began with a few challenges, first is to overcome beginning with acually loading the data for analysis and then memory issues. The first real challenge I had was extracting the ratings data from the ml-10m100K/ratings.dat file which I could not complete on my laptop (Alienware I7 6700hq cpu @2.60Ghz and 16GB with Windows 10 Pro for Workstations, of which I still have no idea why it will not import). After days of attempting then purchasing a faster desktop computer then I was able to sucessfully import. During my initial struggles I also found that using fread function compared to read.table function to be faster considerable faster for reading in the data therefore I altered the initial download process from what was given.

Two tables of data called movies and ratings are provided. The datasets will be joined by movieIds and userIDs to make our MovieLens Dataset. The MovieLens data will then be split with 10 percent as Validation dataset and the remainder as EDX dataset. Make sure userId and movieId in validation set are also in edx set then add the Validation set back into EDX.

```{r Install Packages and libraries, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(useful)) install.packages("useful", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(recommenderlab)) install.packages("recommenderlab", repos = "http://cran.us.r-project.org")
library(dplyr)
library(stringr)
library(tidyverse)
library(caret)
library(data.table)
library(useful)
library(lubridate)
library(recommenderlab)

#Initial data variables
top_movie_n <- 100
users_movie_n <- 70

```
```{r Load Data, include=FALSE}
cat("Movielens Preptime Times:")  
system.time({
# MovieLens 10M dataset:
if(!file.exists("ml-10m.zip")){
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", destfile = "ml-10m.zip")
}
unzip("ml-10m.zip", exdir = "movies")
#dir("movies")
#dir("movies/ml-10m100K/")
ratings  <- fread("ml-10m100K/ratings.dat", sep = ":")[, c(1,3,5,7), with=FALSE]
setnames(ratings, c("userId","movieId","rating", "timestamp"))

movies <- str_split_fixed(readLines("ml-10m100K/movies.dat"), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm( ratings, movies, test_index, temp, movielens, removed)  
})
#original code took 20 minutes to complete this initial dataset process
#Modifing the code to use reduces initial dataset process to approx 1 minute.
```

### Initial Analysis of EDX Data:  
```{r Summary of edx of dataset, echo=FALSE}
edx_rows <- nrow(edx)
edx_cols <- ncol(edx)
edx_names <- names(edx)
summary(edx)   
```
From the  summary of edx dataset we know there are 9000061 row with 6 variables UserId, MovieId, rating, timestamps, title and multiple combinations of genres. It also appears there is no missing data.

```{r Glimpe of edx dataset, echo=FALSE}
glimpse(edx)
```
A glimpse of the data I notice timestamp needs to be converted if I am to do any timeseries related predictions which my current plan does not.
  
```{r Additional intial information, eval=FALSE, include=FALSE}
class(edx)
cat("Size of edx file (bytes) : " , object.size(as(edx,"matrix")), "\n")   
```
####Image of 100 users and 100 movies. 

```{r Top 100 movies by rating counts, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
users <- sample(unique(edx$userId), 100)

edx %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")   

```

In the image above, note each row represents a user and the columns represent movies they rated. Notice that not every user has rated a movie (row 2) in the sample set to some rating numerous movies. Since I plan to use content based filtering for my predictions, I need my good dataset with many users that have rated at many movies. 


Plot edx data for the count of movies rated:  
```{r Get Movie rating counts, echo=FALSE}
edx %>% 
  dplyr::count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Movies Rating Counts")  

```

Looking at the Movies Rating Count plot, shows that the majority of movies have been rated over 200 times.


Plot the count of ratings given by users:  
```{r Get User Rating counts, echo=FALSE}
edx %>% 
  dplyr::count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Users Rating Counts")  

```

Looking at the Users Rating Counts, shows that most users have rated at least **`r users_movie_n`**  movies

### Preparing Data:   
```{r Clean data remove low rating count users, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
keep_user <- edx %>%
  dplyr::count(userId) %>%
  filter(n >= users_movie_n) %>%
  pull(userId)

edx_1 <- edx %>%filter(userId %in% keep_user)  
```
Based on the discovered information, I identify users who have rated 20 movies or more, leaving **`r length(keep_user)`** users. 


```{r Clean data Get top  100 movies, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
keep <- edx %>%
  dplyr::count(movieId) %>%
  top_n(top_movie_n) %>%
  pull(movieId)

edx_1 <- edx_1 %>%filter(movieId %in% keep)  
```
Due to performance issues I use only the top 100 rated movies, giving a dataset with **`r length(keep)`** movies and **`r length(keep_user)`** with **`r dim(edx_1)`** ratings and columns.  

Do to the size of the datasets and reading several articules, I have choosen to use sparse matrix because  "Sparse matracies also have significant advandatages in terms of computational efficiency. Unlike opertations with fill matrices, operatios with sparce matricies do not perform unnessessary low-level arithmetics"  Priyam (2016). The decision to use sparse matrices has lead my to use recommenderLab, Hahsler (2019) which will involve converting my edx data.frame data to matrix to finally realRatingMatrix class.   

```{r Convert edx to realRatingsMatrix, include=FALSE}
#1. I used dcast.data.table to cast the data.frame as a table  
#2. I used sprintf to convert MovieId's, and UserId's to chr  
#3. I used corner to view a small sample of the data to verify conversion  
#4. I then convert the data ta a matrix and then a realRatingMatrix  

edx_2 <- dcast(userId ~ movieId, data = edx_1, value.var = "rating")
dim(edx_2)
class(edx_2)
#view data
require(useful)
  corner(edx_2)

#change rownames
rownames(edx_2) <- sprintf("User%s",edx_2$userId)  
edx_2$userId <- NULL  
corner(edx_2)  

#change column Names
colnames(edx_2) <- sprintf("Movie%s",colnames(edx_2))  
corner(edx_2)  

#convert to matrix  
edx_2 <- as.matrix(edx_2)  
dim(edx_2)  
class(edx_2)  

#convert to realRatingMatrix
edx_3 <- as(edx_2 , "realRatingMatrix")  
class(edx_3)  
str(edx_3)  
```

```{r performance related info, eval=FALSE, include=FALSE}
cat("                Size of edx file : ", object.size(as(edx,"matrix")), " bytes\n")  
cat("          Size of Trimed Dataset : ", object.size(as(edx_1,"matrix")), " bytes\n")  
cat("   Size of Trimed Matrix Dataset : ", object.size(as(edx_2,"matrix")), " bytes\n")  

```

```{r convert Validation dataset realRatingMatrix, include=FALSE}
val_movies <- validation %>%
  dplyr::count(movieId) %>%
  top_n(1000) %>%
  pull(movieId)
val_user <- validation %>%
  dplyr::count(userId) %>%
  filter(n > 10) %>%
  pull(userId)
val_1 <- validation %>%filter(userId %in% val_user)
dim(edx_1)
val_1 <- val_1 %>%filter(movieId %in% val_movies)
dim(val_1)
val_2 <- dcast(userId ~ movieId, data = val_1, value.var = "rating")
dim(val_2)

#view data
require(useful)
  corner(val_2)

#change rownames
rownames(val_2) <- sprintf("User%s",val_2$userId)
val_2$userId <- NULL
corner(val_2)

#change column Names
colnames(val_2) <- sprintf("Movie%s",colnames(val_2))
corner(val_2)

#convert to matrix  
val_2 <- as.matrix(val_2)
#head(edx_2)
#class(edx_2)
dim(val_2)
val_3 <- as(val_2, "realRatingMatrix")
val_data <- val_3[rowCounts(val_3) >70]  
```

####Review the prepared dataset  

```{r Review ratings distribution, echo=FALSE}
hist(getRatings(edx_3), main = "Distribution of Ratings")
```

The Distribution of Ratings shows we have 10 different possible ratings from 0.5 to 5 in incriments of 0.5.

```{r Normalized Distribution of Ratings, echo=FALSE }
hist(getRatings(normalize(edx_3)),breaks = 100,main = "Normalized Distribution of Ratings")  

```

```{r Visual image of distribution of first 500 users, echo=FALSE}
image(edx_3[1:500,],main = "Visual image of distribution of first 500 users")  

```

```{r Avg Ratings of Prepared data, eval=FALSE, include=FALSE}
boxplot(rowMeans(edx_3),main = "Avg Ratings of Prepared data")  

```  

*Due to performance/memory issues, I remove users with less than 70 movie ratings for my model dataset*

```{r Reduce userIds, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#users_movie_n <- 70  #started at 30+ movies rated by user but had performance issues so increased to 70+
```

```{r Create model data, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

model_data <- edx_3[rowCounts(edx_3) > users_movie_n]
model_data
boxplot(rowMeans(model_data), main = "Box Plot of Rating Means")  

```

I can further cleeanup data by removing outlirers, When looking at the previous boxplot of the dataset we have a few outliers with row means below 2.7 and above 4.6 so I will remove them.

Outiers with rowMeans below 2.7 to remove

```{r Remove outliers, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
dim(model_data[rowMeans(model_data) < 2.7 ])   
```

Outiers wit rowMeans above 4.6 to remove

```{r echo=FALSE}
dim(model_data[rowMeans(model_data) > 4.6 ])   
```
```{r include=FALSE}
model_data <- model_data[rowMeans(model_data) >= 2.7 & rowMeans(model_data) <= 4.6 ]  
```

Rows remaining after removing outliers
```{r Remaing data dimisions, echo=FALSE}
dim(model_data)
```

#### After Outliers removed...  
Boxplot of Model Data after outliers removed, data is symetric.

```{r Boxplot after removin outliers, echo=FALSE}
boxplot(rowMeans(model_data))   

```

Number of Rating remaining in Model Data  

```{r Number of ratings after outliers removed, echo=FALSE}
nratings(model_data)  

```

#### Model dataset Distribution of Ratings  

```{r Plot the Model Data movie ratings, echo=FALSE}
hist(getRatings(model_data), main = "Distribution of Ratings after removing Outliers")   

```

Visual of Model Dataset  

```{r image of Model dataset, echo=FALSE}
image(as(model_data,"matrix"), main = "Visual image of Rating distribution - Model Data")  

```

Model data appears to be well ditributed.   


#### Create Modeling datasets  

```{r Create recommender sets, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
set.seed(1)
which_train <- sample( x = c(TRUE, FALSE), size = nrow(model_data), replace = TRUE, prob = c(0.8, 0.2))
head(which_train)
rec_data_train <- model_data[which_train]
rec_data_test <- model_data[!which_train]

```
Divide the Model data set into train and test sets, 80/20 respectively.  

Train set diminsions: `r dim(rec_data_train)`  
Test set diminsions: `r dim(rec_data_test)`  


#### Build Models Options 

```{r Get list of recommender models available, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
recommender_models <-recommenderRegistry$get_entries(dataType = "realRatingMatrix")  
```

```{r show model details, eval=FALSE, include=FALSE}
lapply(recommender_models,"[[","description")   
recommender_models  #Description, references and parameteres of models
```
I will use various models then compare prediction accuracies to determine the best algorithim. The available models I will first use a user-based collaborative filtering algorithm (UBCF), then item-based collaborative filtering (IBCF) and item popularity algorithms (POPULAR).

#### User Based Collborative Filtering (UBCF) Model
Collaborative filtering uses algorithims to filter users ratings to make personalized recommendations from similiar users (definition from whatis.techtarget.com/definition/collaborative-filtering).

```{r Build User Based Collabrotive model, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
recc_model <- Recommender(data = rec_data_train, method = "UBCF")  
recc_model  
recc_model@model$data  
```

Get recommendations for testset:
```{r Get recommender test set, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
# note UBCF model is a lazy learner technic that must access all data in order to make a prediction
  n_recommended <- 10
  recc_predicted <- predict(object = recc_model, newdata = rec_data_test, n = n_recommended)
  rec_list <- sapply(recc_predicted@items, function(x){colnames(model_data)[x]})
```

List of recommendation movies for test set users 7 thru 10:  
```{r List recommendations for a few users, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  rec_list[7:10]  

```
Total number of recommendations by users in test set  
```{r Total number of recommenders in test set, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  number_of_items = sort(unlist(lapply(rec_list, length)), decreasing = TRUE)
  table(number_of_items)
```
Note that 428 users from the test set received 10 recommendations. 

Analyze number of ratings per user
```{r Analyze number of ratings per users, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  table(rowCounts(model_data)) 
```
Note that only one user has rated 96 of the top 100 rated movies. 


#### Create an evaluators scheme:
Create evaluation datasets using recommenderLab evaluationScheme function  

```{r Create Evaluation scheme, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  items_to_keep <- 30
  rating_threshold <- 4
  n_fold <- 5
  eval_sets <- evaluationScheme(data = model_data, method = "cross-validation", train 
                                = percentage_training, given = items_to_keep, goodRating = rating_threshold, k = n_fold)
  eval_sets  
```

```{r Get size of evaluation sets, echo=FALSE}
  size_sets <- sapply(eval_sets@runsTrain, length)   
  cat("Sizes of Evaluation Sets:\t", size_sets, "\n")   
  getData(eval_sets, "train")
```
3 sets will be used:
  train = training set  
  known = test set used to build recommendations   
  unknown = test set to test the recommendations   
  
Create UBCF Recommender 
```{r Evaluate UBCF known, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  model_to_evaluate <-"UBCF"
  model_paramenter <- NULL
  eval_recommender <- Recommender(data = getData(eval_sets, "train"), method = model_to_evaluate, parameter = model_paramenter)
  eval_recommender  
```
Get UBCF Filter predictions for known test set  
```{r Get evaluation predictions, echo=FALSE}
  items_to_recommend <- 10
  eval_prediction <- predict(object = eval_recommender, newdata = getData(eval_sets, "known"), n = items_to_recommend, type = "ratings")
  eval_prediction 
```
Calculate the prediction accuracy for each user in unknown test set:
```{r Calculate the prediction accuracy UBCF, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, data = getData(eval_sets, "unknown"), byUser = TRUE)
  head(eval_accuracy)
```
Calculate the overall avgerages in unknown test set: 
```{r Calculate the accuracy of UBCF Model, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  apply(eval_accuracy,2,mean)
```
Calculate the overall accuracy given in unknown test set:
```{r Calculate the UBCF overall accuracy, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, data = getData(eval_sets, "unknown"), byUser = FALSE)
  eval_accuracy
```
Use precicion recall to predict accuracy with confusion matrix for known test set
```{r Use prediction recall to predict accuarcy, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  results <- evaluate(x = eval_sets, method = model_to_evaluate, n = seq(10, 100, 10))
  head(getConfusionMatrix(results)[[1]])
  # note first 4 columns cotain the True False Positives
```
Sum up the UBCF indexes:
```{r Sum up the UBCF indexes, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#sum up the indexes 
  columns_to_sum <- c("TP","FP","FN","TN")
  indicies_summed <- Reduce("+", getConfusionMatrix(results))[,columns_to_sum]
  indicies_summed  
  
```
*Note: it is difficult to visulize the data provided unless the results are plotted.*


Create UBCF Receiver operating characteristic (ROC) plot
```{r Create UBCF ROC plot, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
  plot(results, annotate = TRUE, main = "UBCF - ROC Curve")  

```

Plot UBCF Precision/recall to verify accuracy
```{r Plot UBCF Precision recall, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#plot shows the relation ship between TPR and FPR
  #30 TPR is close to 0.7 and the FPR is less than 0.4 is good
  #40 TPR is close to 0.7 but the FPR is greater than 0.4 is not as good
  plot(results, "prec/rec",annotate = 1, main = "UBCF - Precision/recall")   

```
*Note the precision/recall at #30 is not the best at 0.58/0.66*


#### Fine Tuning of the Models to get best results

Create UBCF Models with varing vector_nn and different metods i.e.: cosine and pearson

```{r Fine tune UBCF model, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
vector_nn <- c(5,10,20, 30)
UBCF_cos_model <- lapply(vector_nn, function(nn,l){ list(name ="UBCF", param = list(method = "cosine", nn = nn))})
names(UBCF_cos_model) <- paste0("UBCF_cos_k_", vector_nn)
names(UBCF_cos_model)[1]
UBCF_pea_model <- lapply(vector_nn, function(nn,l){ list(name ="UBCF", param = list(method = "pearson", nn = nn))})
names(UBCF_pea_model) <- paste0("UBCF_pea_k_", vector_nn)
names(UBCF_pea_model)[1]
models <- append(UBCF_cos_model, UBCF_pea_model)
models  

```

Determine the best UBCF results based on number of recommendations  

```{r Get UBCF results, echo=FALSE}
n_recommendations <- c(1, 5, seq(10,100,10))
list_results <- evaluate(x = eval_sets, method = models, n = n_recommendations)  

```

Plot UBCF results 

```{r plot UBCF model results, echo=FALSE}
plot(list_results, annotate = c (1,2), legend = "bottomright")
title("UBCF ROC curve")  

```

*Note: UBCF_pea_k_30 appears to be the best UBCF model with TPR closes to 0.7 and FPR less than 0.4*

```{r echo=FALSE}
plot(list_results, "prec/rec",annotate = 1, legend= "bottomleft")
title("UBCF Precision/recall")
```
*Note: The precision/recall support UBCF_pea_k_30 appears to be the best UBCF model with high persision*


Create IBCF Models with varing vector_nn and different metods i.e.: cosine and pearson
```{r Create IBCF model, echo=FALSE}
vector_k <- c(5,10,20,30)
IBCF_cos_model <- lapply(vector_k, function(k,l){ list(name ="IBCF", param = list(method = "cosine", k = k))})
names(IBCF_cos_model) <- paste0("ICBF_cos_k_", vector_k)
names(IBCF_cos_model)[1]
IBCF_pea_model <- lapply(vector_k, function(k,l){ list(name ="IBCF", param = list(method = "pearson", k = k))})
names(IBCF_pea_model) <- paste0("ICBF_pea_k_", vector_k)
names(IBCF_pea_model)
models <- append(IBCF_cos_model, IBCF_pea_model)
models
```

Get IBCF model results 

```{r Get IBCF results, echo=FALSE}
n_recommendations <- c(1, 5, seq(10,100,10))
list_results <- evaluate(x = eval_sets, method = models, n = n_recommendations)   

```
 
 Plot IBCF results   
 
```{r plot IBCF ROC curve, echo=FALSE}
plot(list_results, annotate = c (1,2), legend = "bottomright")
title("IBCF ROC curve")   
```

```{r plot IBCF Precision/recall, echo=FALSE}
plot(list_results, "prec/rec",annotate = 1, legend= "bottomright")
title("Precision/recall")   
  
```

IBCF model #10-40 TPR is closes to to 0.7 and the FPR is less than 0.4 is good

####Create a POPULAR model   

Create POPULAR model   
```{r Create Popular model, echo=FALSE}
vector_nn <- 30
POP_model <- lapply(vector_nn, function(nn,l){ list(name ="POPULAR")})
names(POP_model) <- "POPULAR"
names(POP_model)[1]
models <- POP_model
models  

```

Get POPULAR Results   

```{r Get Popular list results, include=FALSE}
n_recommendations <- c(1, 5, seq(10,100,10))
list_results <- evaluate(x = eval_sets, method = models, n = n_recommendations)
```

Plot POPULAR Results   

```{r Plot Popular ROC Curve, echo=FALSE}
#plot and choose the optimal parameters
plot(list_results, annotate = c (1,2), legend = "topleft")
title("ROC curve")   

```

```{r Plot Popular Precision/recall, echo=FALSE}
plot(list_results, "prec/rec",annotate = 1, legend= "bottomright")
title("Precision/recall")   
```

#### Combine best results from UBCF, IBCF and POPULAR models to determine my Final Model

```{r include=FALSE}
models <- append(UBCF_pea_model, POP_model )
n_recommendations <- c(1, 5, seq(10,100,10))
list_results <- evaluate(x = eval_sets, method = models, n = n_recommendations)  

```

Plot Best Results   

```{r Plot Best ROC Curve, echo=FALSE}
#plot and choose the optimal parameters
plot(list_results, annotate = c (1,2), legend = "topleft")
title("ROC curve")   

```

```{r Plot Best Precision/recall, echo=FALSE}
plot(list_results, "prec/rec",annotate = 1, legend= "bottomright")
title("Precision/recall")   
```


### Evaluate using the Validation dataset

#### Initial Analysis of Validation data:
Dimensions (rows/columns):
```{r Initial view of Validation data, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
dim(validation)
```
Corner view of Validation data:
```{r Corner view of Validation data, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
corner(validation)
```


```
Summary of validation realRatingMatrix dataset:  
```{r echo=FALSE}
cat("Total number of ratings in validation dataset:\t", nratings(val_data),"\n")
cat("Total Users to Movies in validation set:\t", dim(val_data), "\n")   
```

Create Validation Sets
```{r echo=FALSE}
items_to_keep <-30
rating_threshold <- 4
n_fold <- 5
n_recommendations <- c(1, 5, seq(10,100,10))
val_sets <- evaluationScheme(data = val_data, method = "cross-validation", train 
                                = percentage_training, given = items_to_keep, goodRating = rating_threshold, k = n_fold)  
```
Evaluate Validation data with selected models  

Final UBCF Evaluation results:  
```{r Final UBCF Evaluation results, echo=FALSE}
UBCF_eval <- evaluate(x = val_sets, method = "UBCF", k = n_fold, type = "ratings")

```

```{r Final UBCF Results, echo=FALSE}
head(getConfusionMatrix(UBCF_eval)[[1]])  

```

Final IBCF Evaluation results:  
```{r echo=FALSE}
IBCF_eval <- evaluate(x = val_sets, method = "IBCF", k = n_fold, type = "ratings")

```


```{r IBCF Results, echo=FALSE}
head(getConfusionMatrix(IBCF_eval)[[1]])

```

Final POPULAR Evaluation results:  
```{r Final POPULAR Evaluation results, echo=FALSE}
POP_eval <- evaluate(x = val_sets, method = "POPULAR", n = n_recommendations, type = "ratings")

```

```{r Final POPULAR Results, echo=FALSE}
head(getConfusionMatrix(POP_eval)[[1]])
```

### Conclusion

In Conclusion using using the **POPULAR**  model from the recommenderLab library, keeping **30** items with a predicted movie rating of **4** reports an RMSE of **0.872**.
In this capstone prioject I learned various ways to improve speed using different classes of data and different libraries that were not introduced in the class.

```{r File size reduction, echo=FALSE}
cat("The Prepared dataset was reduced by", object.size(as(edx_3, "matrix"))/object.size(edx_3),"to 1 byte when converted from matrix to realRatingsMatrix which greatly reduced time, memory issues and stress.\n")

```


####notes

A downfall User Based Collaborative Filtiering is the user need to have rated a few movies inorder to find similiar users ratings. 


The idea of gathering similiar users based on movies they watched and how they rated the movies then use the total average means to predict the movie ratings.  All this said I begin cleaning the data based on users than have rated alot of movies.  

To cite package ‘recommenderlab’ in publications use:
Michael Hahsler (2019). recommenderlab: Lab for Developing and Testing Recommender Algorithms. R package version 0.2-4. https://github.com/mhahsler/recommenderlab

